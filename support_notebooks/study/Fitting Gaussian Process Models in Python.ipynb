{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://blog.dominodatalab.com/fitting-gaussian-process-models-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Building models with Gaussians"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "What if we choose to use Gaussian distributions to model our data?\n",
    "\n",
    "$$p(x|\\pi,\\Sigma) = (2\\pi)^{-k/2}|\\Sigma|^{-1/2}\\exp{-\\frac{1}{2}(x-\\mu)'\\Sigma^{-1}(x-\\mu)}$$\n",
    "\n",
    "There would not seem to be any gain doing this, because normal dsitributions are not particularly flexible distributions in and of themselves. However, adopting a set of Gaussians (a multivariate normal vector) confers a number of advantages. First, the marginal distribution of any subset of elements from a multivariate normal distribution is also normal:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "p(x,y) & = & \\mathcal{N}\\left(\\left[\\begin{array}{c}\n",
    "\\mu_x\\\\\n",
    "\\mu_y\n",
    "\\end{array}\\right],\\left[\\begin{array}{ccc}\n",
    "\\Sigma_x & \\Sigma_{xy}\\\\\n",
    "\\Sigma_{xy}^T & \\Sigma_y\n",
    "\\end{array}\\right]\\right)\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "$$p(x) = \\int p(x,y) dy = \\mathcal{N}(\\mu_x, \\Sigma_x)$$\n",
    "\n",
    "Also, conditional distributions of a subset of the elements of a multivariate normal distribution (conditional on the remaining elements) are normal too:\n",
    "\n",
    "$$p(x|y) = \\mathcal{N}(\\mu_x + \\Sigma_{xy}\\Sigma_y^{-1}(y-\\mu_y), \\Sigma_x - \\Sigma_{xy}\\Sigma^{-1}_{y}\\Sigma_{xy}^T)$$\n",
    "\n",
    "A Gaussian process generalizes the multivariate normal to infinite dimension. It is defined as an infinite collection of random variables, with any marginal subset having a Gaussian distribution. Thus, the marginalization property is explicit in its definition. Another way of thinking about an inifinite vector is as a function. When we write a function that takes continuous values as inputs, we are essentially implying an inifite vector that inly returns values (indexed by the inputs) when the function is called upon to do so. By the same token, this notion of an infinite-dimensional Gaussian represented as a function allows us to work with them computationally: we are never required to store all the elements of the Gaussian process, only to calculate them on demand.\n",
    "\n",
    "So, we can describe a Gaussian process as a distribution over functions. Just as a multivariate normal distribution is completely specified by a mean vector and covariance matrix, a GP is fully specified by a mean function and a covariance function:\n",
    "\n",
    "$$p(x) \\sim GP(m(x), k(x,x'))$$\n",
    "\n",
    "It is the marginalization property that makes working with a Gaussian process feasible: we can marginalize over the infinitely-many variables that we are not interested in, or have not observed.\n",
    "\n",
    "For example, one specification of a GP might be:\n",
    "\n",
    "$$\\begin{aligned}\n",
    "m(x)&=0 \\\\\n",
    "k(x, x')&=\\theta_1\\exp(-\\frac{\\theta_2}{2}(x-x')^2)\n",
    "\\end{aligned}$$\n",
    "\n",
    "Here, the covariance function is a squared exponential, for which values of $x$ and $x'$ that are close together result in values of $k$ closer to one, while those that are far apart return values closer to zero. It may seem odd to simply adopt the zero function to represent the mean of the Gaussian process - surely we can do better than that. It turns out that most of the learning in the GP involves the covariance function and its hyperparameters, so very little is gained in specifying a complicated mean function.\n",
    "\n",
    "For a finite number of points, the GP becomes a multivariate normal, with the mean and covariance as the mean function and covariance function, respectively, evaluated at those points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sampling from a GP\n",
    "\n",
    "To make this notion of a \"distribution over functions\" more concrete, let's quickly demonstrate how we obtain realizations from a Gaussian process, which results in an evaluations of a function over a set of points. All we will do here is a sample from the prior Gaussian process, so before any data has been introduced. What we need first is our covariance function, which will be the squared exponential, and a function to evaluate the covariance at given points (resulting in a covariance matrix)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "def exponential_cov(x, y, params):\n",
    "    return params[0] * np.exp(-0.5 * params[1] * np.subtract.outer(x,y)**2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We are going to generate realizations sequentially, point by point, using the conditioning property of multivariate Gaussian. Here is the conditional\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "p(x,y) & =  \\mathcal{N}\\left(\\left[\\begin{array}{c}\n",
    "\\mu_x\\\\\n",
    "\\mu_y\n",
    "\\end{array}\\right],\\left[\\begin{array}{ccc}\n",
    "\\Sigma_x & \\Sigma_{xy}\\\\\n",
    "\\Sigma_{xy}^T & \\Sigma_y\n",
    "\\end{array}\\right]\\right)\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "$$p(x|y) = \\mathcal{N}(\\mu_x + \\Sigma_{xy}\\Sigma_y^{-1}(y-\\mu_y), \\Sigma_x - \\Sigma_{xy}\\Sigma^{-1}_{y}\\Sigma_{xy}^T)$$\n",
    "\n",
    "To simplify,\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray*}\n",
    "p(x,y) & = & \\mathcal{N}\\left(\\left[\\begin{array}{c}\n",
    "\\mu_x\\\\\n",
    "\\mu_y\n",
    "\\end{array}\\right],\\left[\\begin{array}{ccc}\n",
    "A & B\\\\\n",
    "C & D\n",
    "\\end{array}\\right]\\right)\n",
    "\\end{eqnarray*}\n",
    "$$\n",
    "\n",
    "$$p(x|y) = \\mathcal{N}(\\mu_x + BD^{-1}(y-\\mu_y), A - BD^{-1}C)$$\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def conditional(x_new, x, y, params):\n",
    "    \n",
    "    A = exponential_cov(x_new, x_new, params)\n",
    "    B = exponential_cov(x_new, x, params)\n",
    "    C = B.T\n",
    "    D = exponential_cov(x, x, params)\n",
    "    \n",
    "    mu = np.linalg.inv(D).dot(B).T.dot(y)\n",
    "    sigma = A - B.dot(np.linalg.inv(D).dot(C))\n",
    "    \n",
    "    return (mu.squeeze(), sigma.squeeze())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will start with a Gaussian Process prior with hyperparameter $\\theta_0=1$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
